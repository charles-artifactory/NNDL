{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_epochs = 50\n",
    "hidden_sizes = [512, 256, 128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Split training data into training and validation sets (80% train, 20% validation)\n",
    "train_size = int(0.8 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "# Data loaders\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# CIFAR-10 classes\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnectedNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, num_classes):\n",
    "        super(FullyConnectedNN, self).__init__()\n",
    "        \n",
    "        # Flatten layer to convert 3D images to 1D vectors\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        # Create a list to hold all layers\n",
    "        layers = []\n",
    "        \n",
    "        # Input layer\n",
    "        layers.append(nn.Linear(input_size, hidden_sizes[0]))\n",
    "        layers.append(nn.ReLU())\n",
    "        \n",
    "        # Hidden layers\n",
    "        for i in range(len(hidden_sizes) - 1):\n",
    "            layers.append(nn.Linear(hidden_sizes[i], hidden_sizes[i+1]))\n",
    "            layers.append(nn.ReLU())\n",
    "        \n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(hidden_sizes[-1], num_classes))\n",
    "        \n",
    "        # Sequential container\n",
    "        self.linear_layers = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        out = self.linear_layers(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CIFAR-10 images are 32x32 with 3 color channels\n",
    "input_size = 32 * 32 * 3\n",
    "num_classes = 10\n",
    "\n",
    "# Initialize the network\n",
    "model = FullyConnectedNN(input_size, hidden_sizes, num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    loss = running_loss / len(data_loader)\n",
    "    accuracy = 100 * correct / total\n",
    "    \n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device):\n",
    "    # Lists to store metrics\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    # Track time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Set model to training mode\n",
    "        model.train()\n",
    "        \n",
    "        # Metrics for this epoch\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        # Create progress bar with specified format\n",
    "        progress_bar = tqdm(\n",
    "            train_loader,\n",
    "            total=len(train_loader),\n",
    "            desc=f'Epoch {epoch+1}/{num_epochs}',\n",
    "        )\n",
    "        \n",
    "        for batch_idx, (images, labels) in enumerate(progress_bar):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            current_loss = running_loss / (batch_idx + 1)\n",
    "            current_acc = 100 * correct / total\n",
    "            \n",
    "            progress_bar.set_postfix_str(\n",
    "                f'loss: {current_loss:.4f} - acc: {current_acc:.2f}%'\n",
    "            )\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_acc = 100 * correct / total\n",
    "        train_losses.append(epoch_loss)\n",
    "        train_accuracies.append(epoch_acc)\n",
    "        \n",
    "        val_loss, val_acc = evaluate_model(model, val_loader, criterion, device)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_acc)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs} - '\n",
    "              f'loss: {epoch_loss:.4f} - acc: {epoch_acc:.2f}% - '\n",
    "              f'val_loss: {val_loss:.4f} - val_acc: {val_acc:.2f}%')\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f'Training completed in {elapsed_time:.2f} seconds')\n",
    "    \n",
    "    return {\n",
    "        'train_losses': train_losses,\n",
    "        'train_accuracies': train_accuracies,\n",
    "        'val_losses': val_losses,\n",
    "        'val_accuracies': val_accuracies\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    class_correct = list(0. for i in range(10))\n",
    "    class_total = list(0. for i in range(10))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            # Per-class accuracy\n",
    "            c = (predicted == labels).squeeze()\n",
    "            for i in range(labels.size(0)):\n",
    "                label = labels[i]\n",
    "                class_correct[label] += c[i].item()\n",
    "                class_total[label] += 1\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Test Accuracy: {accuracy:.2f}%')\n",
    "    \n",
    "    # Print per-class accuracy\n",
    "    for i in range(10):\n",
    "        print(f'Accuracy of {classes[i]}: {100 * class_correct[i] / class_total[i]:.2f}%')\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning curves\n",
    "def plot_learning_curves(history, save_to='learning_curves.png', title='Learning Curves'):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['train_losses'], label='Training Loss')\n",
    "    plt.plot(history['val_losses'], label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'{title} - Loss Curves')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history['train_accuracies'], label='Training Accuracy')\n",
    "    plt.plot(history['val_accuracies'], label='Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.title(f'{title} - Accuracy Curves')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_to)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(\n",
    "    model, \n",
    "    train_loader, \n",
    "    val_loader, \n",
    "    criterion, \n",
    "    optimizer, \n",
    "    num_epochs, \n",
    "    device, \n",
    "    model_path=\"model/cifar10_fcnn.pth\",\n",
    "    learining_curves_path=\"learning_curves.png\",\n",
    "    title=\"Learning Curves\",\n",
    "    weight_decay=0.0  # L2 regularization\n",
    "):\n",
    "    \"\"\"\n",
    "    Perform training and testing of the model with different loss functions, optimizers, and regularization methods.\n",
    "    \"\"\"\n",
    "    if isinstance(optimizer, optim.SGD) or isinstance(optimizer, optim.Adam):\n",
    "        optimizer.param_groups[0]['weight_decay'] = weight_decay\n",
    "\n",
    "    # Train the model\n",
    "    history = train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device)\n",
    "    \n",
    "    # Test the model\n",
    "    test_accuracy = test_model(model, val_loader, device)\n",
    "    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
    "    \n",
    "    # Plot learning curves\n",
    "    plot_learning_curves(history, learining_curves_path, title)\n",
    "\n",
    "    # Save the model\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_functions = [\n",
    "    nn.CrossEntropyLoss(),\n",
    "    nn.MSELoss(),\n",
    "    nn.BCEWithLogitsLoss()\n",
    "]\n",
    "\n",
    "optimizers = {\n",
    "    'Batch GD': optim.SGD(model.parameters(), lr=0.01, momentum=0),\n",
    "    'Online GD (SGD)': optim.SGD(model.parameters(), lr=0.01),\n",
    "    'Mini-Batch GD': optim.SGD(model.parameters(), lr=0.01, momentum=0.9),\n",
    "    'Momentum': optim.SGD(model.parameters(), lr=0.01, momentum=0.9),\n",
    "    'Adagrad': optim.Adagrad(model.parameters(), lr=0.01),\n",
    "    'Adam': optim.Adam(model.parameters(), lr=0.001),\n",
    "    'Adamax': optim.Adamax(model.parameters(), lr=0.001)\n",
    "}\n",
    "\n",
    "weight_decay_values = [0.0, 1e-4, 1e-3]\n",
    "\n",
    "model_path = \"model/cifar10_fcnn\"\n",
    "learining_curves_path = \"result/learning_curves\"\n",
    "\n",
    "for loss_fn in loss_functions:\n",
    "    for optimizer_name, optimizer in optimizers.items():\n",
    "        for weight_decay in weight_decay_values:\n",
    "            print(f\"\\nRunning experiment with {loss_fn.__class__.__name__}, {optimizer_name}, weight_decay={weight_decay}\")\n",
    "\n",
    "            learning_curve_title = f'{loss_fn.__class__.__name__} - {optimizer_name} - WD={weight_decay}'\n",
    "\n",
    "            experiment(\n",
    "                model, \n",
    "                train_loader, \n",
    "                val_loader, \n",
    "                criterion=loss_fn, \n",
    "                optimizer=optimizer, \n",
    "                num_epochs=num_epochs, \n",
    "                device=device, \n",
    "                model_path=f\"{model_path}_{loss_fn.__class__.__name__}_{optimizer.__class__.__name__}_wd{weight_decay}.pth\",\n",
    "                learining_curves_path=f\"{learining_curves_path}_{loss_fn.__class__.__name__}_{optimizer.__class__.__name__}_wd{weight_decay}.png\",\n",
    "                title=learning_curve_title,\n",
    "                weight_decay=weight_decay\n",
    "            )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NNDL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
